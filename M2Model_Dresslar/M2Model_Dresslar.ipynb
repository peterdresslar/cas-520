{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Started\n",
    "!uv pip install pynetlogo matplotlib seaborn pandas\n",
    "\n",
    "# Set the path to your NetLogo installation\n",
    "# Note! This is OS and machine dependent.\n",
    "# Perhaps someone has an NL docker image out there?\n",
    "# See https://pynetlogo.readthedocs.io/en/latest/_docs/pynetlogo.html#pynetlogo.core.NetLogoLink\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2 Assignment: Schelling Segregation Model\n",
    "\n",
    "*CAS 520, Peter Dresslar*\n",
    "\n",
    "For this assignment we will use [pynetlogo](https://pynetlogo.readthedocs.io/) to capture and report simulation outputs. An excellent and relevant example appears [here](https://pynetlogo.readthedocs.io/en/latest/_docs/introduction.html)\n",
    "\n",
    "**Important** pynetlogo will apparently only work reliably with NetLogo 6.3.0. While that presents challenges of its own, it is also important to note the the model we are using, `segregation`, *must* have a version number that matches that version of NetLogo. So, grabbing the latest version of the model may not work without tweaking the file. The model with adjusted version number is in this repository.\n",
    "\n",
    "`pynetlogo` requires a Java install, for which I used `brew install cask temurin` (on Apple silicon). While the jvm path can be overridden, it appears that pynetlogoʻs implementation of JPype works to automatially find the installed JVM from that cask. Note that I wound up having to point to the `app` directory within the Netlogo base installation.\n",
    "\n",
    "## Step 1: First Analysis\n",
    "\n",
    "### Experiment 1: Set density to 80% and similarity wanted to 30% and run the simulation several times.\n",
    "\n",
    "### Experiment 2: Keeping density at 80%, change the similarity threshold to 90%.\n",
    "\n",
    "### Experiment 3: While the model is still running from step 2, slowly move the similarity slider to the left and note when the model behaviour changes and segregation emerges.\n",
    "\n",
    "### Experiment 4: Rerun the simulation a few times with 80% density and similarity threshold at the tipping point you discovered.\n",
    "\n",
    "\n",
    "We will begin Step 1 by building our experiment harness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python architecture: arm64\n",
      "Using JVM at: /Library/Java/JavaVirtualMachines/temurin-24.jdk/Contents/Home/lib/server/libjvm.dylib\n",
      "Using NetLogo at: /Users/peterdresslar/Workspace/NetLogo-6.3.0/app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterdresslar/Workspace/cas-520/.venv/lib/python3.12/site-packages/pynetlogo/core.py:209: UserWarning: could not find default NetLogo extensions folder. Extensions not available\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pynetlogo\n",
    "import os\n",
    "import subprocess\n",
    "import platform\n",
    "\n",
    "base_model = \"./Segregation.nlogo\" # (copied into this directory!)\n",
    "\n",
    "def initialize_netlogo():\n",
    "\n",
    "    ### Some unplesantness for pynetlogo setup:\n",
    "    # Get Java home (should return ARM Java if that's what's installed)\n",
    "    java_home = subprocess.check_output(['/usr/libexec/java_home']).decode().strip()\n",
    "    jvm_path = os.path.join(java_home, 'lib', 'server', 'libjvm.dylib')\n",
    "    netlogo_path = '/Users/peterdresslar/Workspace/NetLogo-6.3.0/app'\n",
    "\n",
    "    # Print for verification\n",
    "    print(f\"Python architecture: {platform.machine()}\")\n",
    "    print(f\"Using JVM at: {jvm_path}\")\n",
    "    print(f\"Using NetLogo at: {netlogo_path}\")\n",
    "\n",
    "    # Get a netlogo instance\n",
    "    netlogo = pynetlogo.NetLogoLink(\n",
    "        gui=False,  # cannot set to true for macs\n",
    "        netlogo_home=netlogo_path,\n",
    "        # jvm_path=jvm_path\n",
    "    )\n",
    "\n",
    "    return netlogo\n",
    "\n",
    "def find_breakpoint(netlogo, model, precision):\n",
    "    \"\"\"\n",
    "    This is tricky! We start with the given density and similarity wanted.\n",
    "    We know from manual testing that there is a breakpoint at which point the model \n",
    "    will reduce to 0% unhappy. We also can observe that this \"collapse\" occurs within\n",
    "    1000 ticks, and that the go procedure will halt the model at that point.\n",
    "    \n",
    "    We can run a simple binary \"search\" to find this point,\n",
    "    by running the model with similarity-wanted values between 1.0 and 90.0.\n",
    "\n",
    "    If the model stops within 1000 ticks, we know that the breakpoint is higher than the \n",
    "    searched similarity-wanted value. If the model runs the full 1000 steps, we know that\n",
    "    the breakpoint is lower than the searched similarity-wanted value.\n",
    "\n",
    "    Since we are setting values programatically, we have the luxury of looking beyond the integer values of similarity-wanted.\n",
    "\n",
    "    args:\n",
    "        netlogo: the netlogo instance\n",
    "        model: the model to run\n",
    "\n",
    "    returns:\n",
    "        breakpoint: the similarity-wanted value at which the model breaks down\n",
    "\n",
    "    \"\"\"\n",
    "    density = 80.0\n",
    "    similarity_wanted = 90.0\n",
    "    upper_bound = similarity_wanted\n",
    "    lower_bound = 1.0 # not sure how the model would deal with zeroes!\n",
    "    breakpoint = None\n",
    "\n",
    "    step_size = 10 ** (-precision)  # 10e-precision, in other words.\n",
    "    \n",
    "    iteration = 0\n",
    "    while breakpoint is None:\n",
    "        iteration += 1\n",
    "        midpoint = round((upper_bound + lower_bound) / 2, 6) # round to 4 decimal places\n",
    "        print(f\"Iteration {iteration} at {midpoint}\")\n",
    "        netlogo.load_model(model)\n",
    "        netlogo.command(f\"set density {density}\")\n",
    "        netlogo.command(f\"set %-similar-wanted {midpoint}\")\n",
    "        netlogo.command(\"setup\")\n",
    "        \n",
    "        netlogo.command(\"repeat 1000 [go]\")\n",
    "        # we should be able to see if the model halted early by checking the step number \n",
    "\n",
    "        ticks = netlogo.report(\"ticks\")\n",
    "        print(f\"Ticks: {ticks}\")\n",
    "\n",
    "        if ticks < 1000:\n",
    "            lower_bound = midpoint + step_size\n",
    "        else:\n",
    "            upper_bound = midpoint - step_size\n",
    "\n",
    "        if upper_bound - lower_bound < step_size:\n",
    "            breakpoint = midpoint\n",
    "    \n",
    "    return breakpoint\n",
    "        \n",
    "    \n",
    "def cluster_analysis(data):\n",
    "    \"\"\"\n",
    "    Our experiment data will include the a data frame with turtles of two colors:\n",
    "\n",
    "\n",
    "\n",
    "    The model calls these \"blue\" and \"orange\", respectively.\n",
    "\n",
    "    We will measure the clumpyness of the physical data using a standard pandas cluster analysis.\n",
    "    \"\"\"\n",
    "    results = None\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "def run_experiment(netlogo, model, experiment_name, number_of_runs, density, percent_similar_wanted, max_ticks):\n",
    "    \"\"\"\n",
    "    Run the model with the given parameters and return the results.\n",
    "\n",
    "    args:\n",
    "        netlogo: the netlogo instance\n",
    "        model: the model to run\n",
    "        experiment_name: the name of the experiment\n",
    "        number_of_runs: the number of times to run the model\n",
    "        density: the density of the model\n",
    "        similarity_wanted: the similarity wanted of the model\n",
    "        max_ticks: the maximum number of ticks to run the model\n",
    "\n",
    "    returns:\n",
    "        data: a dictionary containing experiment results\n",
    "        includes:\n",
    "            - number of runs\n",
    "            - density\n",
    "            - similarity wanted\n",
    "            - average average similarity values\n",
    "            - average time to happiness, in ticks (or N/A if model never halts)\n",
    "            - cluster analysis results\n",
    "\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        \"experiment_name\": experiment_name,\n",
    "        \"number_of_runs\": number_of_runs,\n",
    "        \"density\": density,\n",
    "        \"percent_similar_wanted\": percent_similar_wanted,\n",
    "        \"average_similarity_values\": [],\n",
    "        \"time_to_happiness_values\": [],\n",
    "        \"cluster_analysis_values\": [],\n",
    "        \"average_average_similarity_values\": [],\n",
    "        \"average_time_to_happiness\": [],\n",
    "        \"average_cluster_analysis_values\": []\n",
    "    }\n",
    "\n",
    "    # collectors\n",
    "    time_to_happiness_values = []\n",
    "    average_similarity_values = []\n",
    "    cluster_analysis_values = []\n",
    "\n",
    "    for run, _ in enumerate(range(number_of_runs), 1):   # is there an easier way to do this? yes! I :heart enumerate.\n",
    "        print(f\"Experiment name {experiment_name}, Run {run} of {number_of_runs}\")\n",
    "\n",
    "        netlogo.load_model(model)\n",
    "        \n",
    "        netlogo.command(f\"set density {density}\")\n",
    "        netlogo.command(f\"set %-similar-wanted {percent_similar_wanted}\")\n",
    "        netlogo.command(\"setup\")\n",
    "        \n",
    "        # run the model. it will halt from inside netlogo if it meets the stopping condition (no unhappy turtles)\n",
    "        netlogo.command(f\"repeat {max_ticks} [go]\")\n",
    "\n",
    "        ticks = netlogo.report(\"ticks\")\n",
    "        if ticks < max_ticks:   # we will assume that the model did not achieve happiness on the very last tick!\n",
    "            run_time_to_happiness = ticks\n",
    "        else:\n",
    "            run_time_to_happiness = None\n",
    "\n",
    "        run_average_similarity = netlogo.report(\"percent-similar\") \n",
    "        # cluster analysis\n",
    "        # turtle_data = netlogo.report(\"[list who xcor ycor color] of turtles\")\n",
    "        turtle_data = 1\n",
    "        run_cluster_analysis = cluster_analysis(turtle_data)  # need to write function\n",
    "\n",
    "        ticks_print = ticks if ticks else \"N/A\"   # donʻt break the print statement\n",
    "        print(f\" Run {run} complete, ticks: {ticks_print}, average similarity: {run_average_similarity}, cluster analysis: {run_cluster_analysis}\")\n",
    "\n",
    "        # store to collectors\n",
    "        time_to_happiness_values.append(run_time_to_happiness)\n",
    "        average_similarity_values.append(run_average_similarity)\n",
    "        cluster_analysis_values.append(run_cluster_analysis)\n",
    "\n",
    "    # move collectors to data\n",
    "    data[\"time_to_happiness_values\"] = time_to_happiness_values\n",
    "    data[\"average_similarity_values\"] = average_similarity_values\n",
    "    data[\"cluster_analysis_values\"] = cluster_analysis_values\n",
    "\n",
    "    # letʻs do some post-processing\n",
    "    data[\"average_time_to_happiness\"] = sum(time_to_happiness_values) / len(time_to_happiness_values)\n",
    "    data[\"average_average_similarity_values\"] = sum(average_similarity_values) / len(average_similarity_values)\n",
    "    data[\"average_cluster_analysis_values\"] = sum(data[\"average_cluster_analysis_values\"]) / len(data[\"average_cluster_analysis_values\"])\n",
    "\n",
    "    print(f\"Experiment {experiment_name} complete, average time to happiness: {data['average_time_to_happiness']}, average average similarity: {data['average_average_similarity_values']}, average cluster analysis: {data['average_cluster_analysis_values']}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Since we are initializing netlogo here, you must run this cell before running the experiments.\n",
    "\n",
    "netlogo = initialize_netlogo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to run our experiments! Letʻs start with Experiment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name Experiment 1, Run 1 of 2\n",
      " Run 1 complete, ticks: 10.0, average similarity: 71.71296984103402, cluster analysis: 1\n",
      "Experiment name Experiment 1, Run 2 of 2\n",
      " Run 2 complete, ticks: 11.0, average similarity: 72.0706021951943, cluster analysis: 1\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mZeroDivisionError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m experiment_1 = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetlogo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mExperiment 1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdensity\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpercent_similar_wanted\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ticks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(netlogo, model, experiment_name, number_of_runs, density, percent_similar_wanted, max_ticks)\u001b[39m\n\u001b[32m    187\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33maverage_time_to_happiness\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28msum\u001b[39m(time_to_happiness_values) / \u001b[38;5;28mlen\u001b[39m(time_to_happiness_values)\n\u001b[32m    188\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33maverage_average_similarity_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28msum\u001b[39m(average_similarity_values) / \u001b[38;5;28mlen\u001b[39m(average_similarity_values)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m data[\u001b[33m\"\u001b[39m\u001b[33maverage_cluster_analysis_values\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maverage_cluster_analysis_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maverage_cluster_analysis_values\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExperiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperiment_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m complete, average time to happiness: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[33m'\u001b[39m\u001b[33maverage_time_to_happiness\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, average average similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[33m'\u001b[39m\u001b[33maverage_average_similarity_values\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, average cluster analysis: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata[\u001b[33m'\u001b[39m\u001b[33maverage_cluster_analysis_values\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mZeroDivisionError\u001b[39m: division by zero"
     ]
    }
   ],
   "source": [
    "experiment_1 = run_experiment(netlogo, base_model, experiment_name=\"Experiment 1\", number_of_runs=2, density=80, percent_similar_wanted=30, max_ticks=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
